{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "trainWordCnn.ipynb",
   "provenance": []
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "MBAujeY1GJoA",
    "colab_type": "text"
   },
   "source": [
    "## WordCNN\n",
    "This is a model from github:\n",
    "It is originally written in **Tensorflow**. In this Repo, I update the architecture using\n",
    "**Keras**\n",
    "\n",
    "**Author:** Lenin G. Falconi\n",
    "\n",
    "**Date:** May 2020.\n",
    "\n",
    "**email:** enteatenea@gmail.com, lenin.falconi@epn.edu.ec\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j-gj1e8nGcEx",
    "colab_type": "code",
    "outputId": "f7014e7a-427e-4827-91ec-edf56acedfaa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "pip install wget\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "adQHQpHkGo5q",
    "colab_type": "code",
    "outputId": "5e00247a-a797-4810-f289-1d47d4260845",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "pip install nltk"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N5zhEbcTGp4b",
    "colab_type": "code",
    "outputId": "0ae35bd4-6afe-47e2-f10c-6a40066f9264",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% importing libraries\n"
    },
    "id": "hy6bv0lBGJoB",
    "colab_type": "code",
    "outputId": "f322f551-0595-48a3-9991-09e051141d79",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "from sklearn.model_selection import  train_test_split\n",
    "print(tf.__version__)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxEzuGv-U1tq",
    "colab_type": "text"
   },
   "source": [
    "## TPU Strategy\n",
    "The TPU strategy is declared to train this model using TPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yz1h3IPwU8I4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "outputId": "812651a7-ceb0-4fc7-a8ef-11e27fc46ba4"
   },
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.17.171.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.17.171.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.17.171.250:8470\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.17.171.250:8470\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fo_EkZFoGJoG",
    "colab_type": "text"
   },
   "source": [
    "The following are constants in the project"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PtaW9xCNGJoG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "NUM_CLASS = 14\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "WORD_MAX_LEN = 100\n",
    "CHAR_MAX_LEN = 1014"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "wo7VRCRzGJoK",
    "colab_type": "text"
   },
   "source": [
    "## Downloading the dataset\n",
    "The script data_utils.py has some functions that allow to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2t0h7-K9GJoL",
    "colab_type": "code",
    "outputId": "457fe165-b3ed-4415-93ee-1741c06e46df",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    }
   },
   "source": [
    "if not os.path.exists(\"dbpedia_csv\"):\n",
    "    print(\"Downloading dbpedia dataset...\")\n",
    "    download_dbpedia()\n",
    "print(\"Creating dataset\")\n",
    "word_dict = build_word_dict()\n",
    "vocabulary_size = len(word_dict)\n",
    "x, y = build_word_dataset(\"train\", word_dict, WORD_MAX_LEN)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15)\n",
    "train_x = np.array(train_x)\n",
    "valid_x = np.array(valid_x)\n",
    "train_y = np.array(train_y)\n",
    "valid_y = np.array(valid_y)\n",
    "\n",
    "print(\"train and valid datasets created ...\")\n",
    "print(\"train x: {}, x[0]: {}, type:{}\".format(train_x.shape, train_x[0], type(train_x[0])))\n",
    "print(\"valid x: {}\".format(np.shape(valid_x)))\n",
    "print(\"train y: {}\".format(np.shape(train_y)))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading dbpedia dataset...\n",
      "Creating dataset\n",
      "train and valid datasets created ...\n",
      "train x: (476000, 100), x[0]: [124631 318195      9   3705  34668   3705  24695     10     11      6\n",
      "    742    322    857      4    773     22     11     31     15  30623\n",
      " 153733     49  12767 318196     22     11   3565      4   2665    888\n",
      "  44696      8     17   2597      4   5730     19  37685  26297    653\n",
      "     25      4   3575     22      7    877      4  10716     15      6\n",
      "   6134    857      2      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0], type:<class 'numpy.ndarray'>\n",
      "valid x: (84000, 100)\n",
      "train y: (476000,)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "c2WBYO8bGJoQ",
    "colab_type": "text"
   },
   "source": [
    "## Declaring the ConvNet Model\n",
    "Using the Keras Functional API, this section implements the\n",
    "WordCNN model as a function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uJ6KwDA4GJoQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def word_cnn_model_create(embedding_size=128,\n",
    "                          num_filters=100,\n",
    "                          filter_sizes=[3, 4, 5],\n",
    "                          num_classes=14,\n",
    "                          document_max_len=100):\n",
    "    x = tf.keras.Input(shape=(100, ))\n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocabulary_size,\n",
    "                                           output_dim=embedding_size,\n",
    "                                           input_length=document_max_len,\n",
    "                                           embeddings_initializer='uniform')(x)\n",
    "    x_emb = tf.keras.layers.Reshape((100, 128, 1))(embeddings)\n",
    "    pooled_outputs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = tf.keras.layers.Conv2D(input_shape=(None, 100, 128, 1),\n",
    "                                      filters=num_filters,\n",
    "                                      kernel_size=[filter_size, embedding_size],\n",
    "                                      strides=(1, 1),\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")(x_emb)\n",
    "        pool = tf.keras.layers.MaxPooling2D(pool_size=[document_max_len - filter_size + 1, 1],\n",
    "                                            strides=(1, 1),\n",
    "                                            padding='valid')(conv)\n",
    "        pooled_outputs.append(pool)\n",
    "\n",
    "    h_pool = tf.keras.layers.concatenate(pooled_outputs)\n",
    "    h_pool_flat = tf.keras.layers.Flatten()(h_pool)\n",
    "    h_drop = tf.keras.layers.Dropout(rate=0.5)(h_pool_flat)\n",
    "    output = tf.keras.layers.Dense(units=num_classes, activation=\"softmax\")(h_drop)\n",
    "\n",
    "    model = tf.keras.Model(inputs=x, outputs=output)\n",
    "    return model"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rbexWj4MGJoU",
    "colab_type": "text"
   },
   "source": [
    "The previous defined function creates a Model CNN. Some constants are required\n",
    "to the model work. The structure of the model is printed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cQc56qtgGJoV",
    "colab_type": "code",
    "outputId": "702874c1-1633-4f0f-876c-6a5f6e7a2077",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    }
   },
   "source": [
    "embedding_size = 128\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_class = 14\n",
    "\n",
    "with strategy.scope():\n",
    "  wordCNNModel = word_cnn_model_create(embedding_size=embedding_size,\n",
    "                                     num_filters=num_filters,\n",
    "                                     num_classes=num_class,\n",
    "                                     filter_sizes=filter_sizes,\n",
    "                                     document_max_len=WORD_MAX_LEN\n",
    "                                     )\n",
    "  wordCNNModel.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "wordCNNModel.summary()"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 128)     72109312    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 128, 1)  0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 98, 1, 100)   38500       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 97, 1, 100)   51300       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 96, 1, 100)   64100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 300)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 14)           4214        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 72,267,426\n",
      "Trainable params: 72,267,426\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model\n",
    "Training is started by calling the fit method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"training started\")\n",
    "wordCNNModel.fit(x=train_x,\n",
    "                 y=train_y,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=NUM_EPOCHS,\n",
    "                 verbose=1)\n",
    "print(\"training finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the model\n",
    "Using the evaluation method and valid dataset, model can be evaluated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, acc = wordCNNModel.evaluate(x=valid_x, y=valid_y, verbose=1)\n",
    "print(\"loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fkHe895DGJoZ",
    "colab_type": "text"
   },
   "source": [
    "loss, acc = wordCNNModel.evaluate(x=valid_x, y=valid_y, verbose=1)\n",
    "print(\"loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E59JArqVGJoa",
    "colab_type": "code",
    "outputId": "5c24ea23-3e8d-4c68-d1fd-9fd03d4bc517",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    }
   },
   "source": [
    "print(\"training started\")\n",
    "wordCNNModel.fit(x=train_x,\n",
    "                 y=train_y,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=NUM_EPOCHS,\n",
    "                 verbose=1)\n",
    "print(\"training finished\")"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "training started\n",
      "Epoch 1/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9917 - loss: 1.7631\n",
      "Epoch 2/10\n",
      "7438/7438 [==============================] - 255s 34ms/step - sparse_categorical_accuracy: 0.9916 - loss: 1.7632\n",
      "Epoch 3/10\n",
      "7438/7438 [==============================] - 257s 35ms/step - sparse_categorical_accuracy: 0.9919 - loss: 1.7630\n",
      "Epoch 4/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9920 - loss: 1.7629\n",
      "Epoch 5/10\n",
      "7438/7438 [==============================] - 254s 34ms/step - sparse_categorical_accuracy: 0.9920 - loss: 1.7628\n",
      "Epoch 6/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9922 - loss: 1.7627\n",
      "Epoch 7/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9920 - loss: 1.7628\n",
      "Epoch 8/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9922 - loss: 1.7627\n",
      "Epoch 9/10\n",
      "7438/7438 [==============================] - 256s 34ms/step - sparse_categorical_accuracy: 0.9922 - loss: 1.7627\n",
      "Epoch 10/10\n",
      "7438/7438 [==============================] - 255s 34ms/step - sparse_categorical_accuracy: 0.9923 - loss: 1.7626\n",
      "training finished\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuaY0GQpUazY",
    "colab_type": "text"
   },
   "source": [
    "## Evaluating the model\n",
    "Using the evaluation method and valid dataset, model can be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PrpGnYr1u9N5",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "2042cf5a-7849-4edd-df84-6cdae4b8dc6f"
   },
   "source": [
    "loss, acc = wordCNNModel.evaluate(x=valid_x, y=valid_y, verbose=1)\n",
    "print(\"loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 20s 8ms/step - sparse_categorical_accuracy: 0.9852 - loss: 1.7697\n",
      "loss: 1.7697, acc: 0.9852\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}