{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WordCNN\n",
    "This is a model from github:\n",
    "It is originally written in **Tensorflow**. In this Repo, I update the architecture using\n",
    "**Keras**\n",
    "\n",
    "**Author: ** Lenin G. Falconi\n",
    "**Date: ** May 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "from sklearn.model_selection import  train_test_split\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% importing libraries\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following are constants in the project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_CLASS = 14\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "WORD_MAX_LEN = 100\n",
    "CHAR_MAX_LEN = 1014"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading the dataset\n",
    "The script data_utils.py has some functions that allow to download the dataset\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(\"dbpedia_csv\"):\n",
    "    print(\"Downloading dbpedia dataset...\")\n",
    "    download_dbpedia()\n",
    "print(\"Creating dataset\")\n",
    "word_dict = build_word_dict()\n",
    "vocabulary_size = len(word_dict)\n",
    "x, y = build_word_dataset(\"train\", word_dict, WORD_MAX_LEN)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15)\n",
    "train_x = np.array(train_x)\n",
    "valid_x = np.array(valid_x)\n",
    "train_y = np.array(train_y)\n",
    "valid_y = np.array(valid_y)\n",
    "\n",
    "print(\"train and valid datasets created ...\")\n",
    "print(\"train x: {}, x[0]: {}, type:{}\".format(train_x.shape, train_x[0], type(train_x[0])))\n",
    "print(\"valid x: {}\".format(np.shape(valid_x)))\n",
    "print(\"train y: {}\".format(np.shape(train_y)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declaring the ConvNet Model\n",
    "Using the Keras Functional API, this section implements the \n",
    "WordCNN model as a function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_cnn_model_create(embedding_size=128,\n",
    "                          num_filters=100,\n",
    "                          filter_sizes=[3, 4, 5],\n",
    "                          num_classes=14,\n",
    "                          document_max_len=100):\n",
    "    x = tf.keras.Input(shape=(100, ))\n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocabulary_size,\n",
    "                                           output_dim=embedding_size,\n",
    "                                           input_length=document_max_len,\n",
    "                                           embeddings_initializer='uniform')(x)\n",
    "    x_emb = tf.keras.layers.Reshape((100, 128, 1))(embeddings)\n",
    "    pooled_outputs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = tf.keras.layers.Conv2D(input_shape=(None, 100, 128, 1),\n",
    "                                      filters=num_filters,\n",
    "                                      kernel_size=[filter_size, embedding_size],\n",
    "                                      strides=(1, 1),\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")(x_emb)\n",
    "        pool = tf.keras.layers.MaxPooling2D(pool_size=[document_max_len - filter_size + 1, 1],\n",
    "                                            strides=(1, 1),\n",
    "                                            padding='valid')(conv)\n",
    "        pooled_outputs.append(pool)\n",
    "\n",
    "    h_pool = tf.keras.layers.concatenate(pooled_outputs)\n",
    "    h_pool_flat = tf.keras.layers.Flatten()(h_pool)\n",
    "    h_drop = tf.keras.layers.Dropout(rate=0.5)(h_pool_flat)\n",
    "    output = tf.keras.layers.Dense(units=num_classes, activation=\"softmax\")(h_drop)\n",
    "\n",
    "    model = tf.keras.Model(inputs=x, outputs=output)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The previous defined function creates a Model CNN. Some constants are required\n",
    "to the model work. The structure of the model is printed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_class = 14\n",
    "wordCNNModel = word_cnn_model_create(embedding_size=embedding_size,\n",
    "                                     num_filters=num_filters,\n",
    "                                     num_classes=num_class,\n",
    "                                     filter_sizes=filter_sizes,\n",
    "                                     document_max_len=WORD_MAX_LEN\n",
    "                                     )\n",
    "wordCNNModel.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                     loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                     metrics=['acc'])\n",
    "\n",
    "wordCNNModel.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training is started by calling the fit method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"training started\")\n",
    "wordCNNModel.fit(x=train_x,\n",
    "                 y=train_y,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=NUM_EPOCHS,\n",
    "                 verbose=1)\n",
    "print(\"training finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}